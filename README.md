<div align="center">

# ğŸ¤– Organization-Specific RAG Assistant Bot

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100.0+-009688.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-ready-2496ED.svg)](https://www.docker.com/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**Enterprise-Grade Retrieval-Augmented Generation Chatbot for Intelligent Organization Assistance**

[ğŸš€ Features](#-key-features) â€¢ [ğŸ“‹ Installation](#-installation) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢ [ğŸ“š Documentation](#-api-documentation) â€¢ [âš¡ Quick Start](#-quick-start) â€¢ [ğŸ§ª Evaluation](#-evaluation)

---

</div>

## ğŸŒŸ Overview

An advanced **Retrieval-Augmented Generation (RAG)** chatbot designed for organizations seeking intelligent, context-aware customer support and information retrieval. Built with enterprise-grade architecture, this system combines cutting-edge NLP techniques with scalable infrastructure to deliver accurate, real-time responses about organizational services, expertise, and capabilities.

**Perfect for:** Customer service automation, internal knowledge management, lead qualification, and 24/7 intelligent assistance.

---

## ğŸš€ Key Features

### ğŸ¯ **Core Capabilities**
- **ğŸ” Intelligent Document Retrieval** - FAISS-powered semantic search with re-ranking
- **ğŸ§  Advanced RAG Pipeline** - End-to-end processing from scraping to generation
- **âš¡ Real-time Responses** - Sub-second query processing with context awareness
- **ğŸ¨ Modern UI/UX** - Clean Streamlit interface with interactive elements
- **ğŸ“Š Built-in Analytics** - Query logging and performance evaluation

### ğŸ› ï¸ **Technical Excellence**
- **ğŸ—ï¸ Modular Architecture** - Microservices-ready with clean separation of concerns
- **ğŸ”§ Highly Configurable** - Environment-based configuration management
- **ğŸ³ Docker Ready** - Containerized deployment for easy scalability
- **ğŸ“ˆ Performance Optimized** - GPU acceleration and efficient memory usage
- **ğŸ§ª Comprehensive Testing** - Built-in evaluation framework with metrics

### ğŸŒ **Enterprise Features**
- **ğŸ”’ Production Ready** - Robust error handling and logging
- **ğŸ“± API-First Design** - RESTful APIs for easy integration
- **ğŸ›ï¸ Intent Classification** - Smart query routing and handling
- **ğŸ“‹ Context Validation** - Relevance checking for accurate responses
- **ğŸ”„ Scalable Processing** - Efficient batch processing and caching

---

## ğŸ—ï¸ Architecture

### System Overview
```mermaid
flowchart TD
    A[ğŸŒ Web Scraper] --> B[ğŸ“ Preprocessor]
    B --> C[âœ‚ï¸ Text Chunker]
    C --> D[ğŸ§  Embedding Model]
    D --> E[ğŸ—„ï¸ FAISS Vector Store]
    
    F[ğŸ‘¤ User Query] --> G[ğŸ¯ Intent Classifier]
    G --> H[ğŸ” Semantic Search]
    H --> I[ğŸ“Š CrossEncoder Re-ranking]
    I --> J[âœ… Context Relevance Check]
    J --> K[ğŸ¤– LLM Generation]
    K --> L[ğŸ“¤ Response]
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style K fill:#e8f5e8
    style L fill:#fff3e0
```

### Data Flow Pipeline
```mermaid
graph LR
    A[Raw Web Data] --> B[Structured JSON]
    B --> C[Preprocessed Chunks]
    C --> D[Vector Embeddings]
    D --> E[FAISS Index]
    E --> F[Semantic Retrieval]
    F --> G[Generated Response]
```

---

## ğŸ“ Project Structure

```
Organization-Specific-RAG-Assistant-Bot/
â”œâ”€â”€ ğŸ–¥ï¸  app/                          # Frontend Application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ streamlit_app.py              # Main Streamlit UI
â”œâ”€â”€ ğŸš€ backend/                       # Backend Services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                     # Backend configuration
â”‚   â”œâ”€â”€ main.py                       # FastAPI application
â”‚   â”œâ”€â”€ logger.py                     # Logging utilities
â”‚   â”œâ”€â”€ ğŸ§  core/                      # Core Logic
â”‚   â”‚   â”œâ”€â”€ intent_classifier.py      # Query intent detection
â”‚   â”‚   â””â”€â”€ rag_pipeline.py          # Main RAG implementation
â”‚   â”œâ”€â”€ ğŸ“‹ schemas/                   # Data Models
â”‚   â”‚   â”œâ”€â”€ pydantic_input.py        # Input validation
â”‚   â”‚   â””â”€â”€ pydantic_output.py       # Output formatting
â”‚   â””â”€â”€ ğŸ› ï¸  utils/                    # Utility Functions
â”‚       â”œâ”€â”€ call_llm.py              # LLM interaction
â”‚       â””â”€â”€ context_relevance_checker.py
â”œâ”€â”€ ğŸ’¾ data/                          # Data Storage
â”‚   â”œâ”€â”€ accumulated_data/            # Structured organizational data
â”‚   â”œâ”€â”€ preprocessed_data/           # Cleaned text chunks
â”‚   â””â”€â”€ scraped_data/               # Raw scraped content
â”œâ”€â”€ ğŸ§® embeddings/                    # Vector Store
â”‚   â”œâ”€â”€ faiss_index_itobuz.index    # FAISS vector index
â”‚   â”œâ”€â”€ chunk_metadata_itobuz.pkl   # Chunk metadata
â”‚   â””â”€â”€ id_mapping_itobuz.db        # ID mappings
â”œâ”€â”€ ğŸ§ª eval/                         # Evaluation Framework
â”‚   â”œâ”€â”€ eval_rag.py                 # Performance evaluation
â”‚   â””â”€â”€ test_queries.jsonl          # Test query dataset
â”œâ”€â”€ ğŸ“Š logs/                         # Application Logs
â”œâ”€â”€ ğŸ•·ï¸  website_scraper/             # Advanced Web Scraping
â”‚   â”œâ”€â”€ cli.py                      # Command-line interface
â”‚   â”œâ”€â”€ scraper.py                  # Main scraper logic
â”‚   â”œâ”€â”€ settings.py                 # Scraper configuration
â”‚   â”œâ”€â”€ spiders/                    # Scrapy spiders
â”‚   â””â”€â”€ utils/                      # Helper utilities
â”œâ”€â”€ ğŸ³ Dockerfile                    # Container configuration
â”œâ”€â”€ âš™ï¸  config.py                    # Global configuration
â”œâ”€â”€ ğŸš€ launch.py                     # Application launcher
â””â”€â”€ ğŸ“‹ requirements.txt              # Dependencies
```

---

## ğŸ“‹ Installation

### Prerequisites
- **Python 3.10+** (3.11 recommended)
- **CUDA-compatible GPU** (optional, for acceleration)
- **Docker** (optional, for containerized deployment)
- **8GB+ RAM** (16GB recommended for optimal performance)

### ğŸ”§ Environment Setup

```bash
# Clone the repository
git clone https://github.com/satyaki-mitra/Organization-Specific-RAG-Assistant-Bot.git
cd Organization-Specific-RAG-Assistant-Bot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download required NLTK data
python -c "import nltk; nltk.download('punkt')"

# Download spaCy model
python -m spacy download en_core_web_sm
```

### ğŸ³ Docker Deployment

```bash
# Build the Docker image
docker build -t rag-assistant .

# Run the container
docker run -p 8000:8000 -p 8501:8501 rag-assistant
```

---

## âš¡ Quick Start

### 1. ğŸ•·ï¸ Data Collection & Preprocessing

```bash
# Step 1: Scrape organizational website
python run_scraper.py

# Step 2: Preprocess and structure data
python preprocess.py

# Step 3: Generate embeddings and build vector index
python embedder.py
```

### 2. ğŸš€ Launch Application

```bash
# Option 1: Launch both backend and frontend
python launch.py

# Option 2: Launch services separately
# Terminal 1 - Backend API
cd backend && python main.py

# Terminal 2 - Frontend UI
streamlit run app/streamlit_app.py
```

### 3. ğŸŒ Access Your Assistant

- **ğŸ–¥ï¸ Web Interface**: [http://localhost:8501](http://localhost:8501)
- **ğŸ”— API Documentation**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ğŸ“Š Health Check**: [http://localhost:8000/health](http://localhost:8000/health)

---

## ğŸ›ï¸ Configuration

### Core Settings (`config.py`)

```python
# Model Configuration
EMBEDDING_MODEL = "intfloat/e5-large-v2"        # Embedding model
LLM_NAME = "HuggingFaceH4/zephyr-7b-alpha"      # Language model
RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Retrieval Parameters
TOP_K = 5                    # Number of chunks to retrieve
SCORE_THRESHOLD = 0.75       # Minimum relevance score
MAX_TOKENS_PER_CHUNK = 512   # Chunk size limit

# API Configuration
API_PORT = 8000             # Backend port
STREAMLIT_PORT = 8501       # Frontend port
LOG_LEVEL = "INFO"          # Logging level

# Performance Settings
USE_GPU = True              # Enable GPU acceleration
BATCH_SIZE = 32            # Processing batch size
CACHE_SIZE = 1000          # Response cache size
```

---

## ğŸ“š API Documentation

### Core Endpoints

#### ğŸ’¬ Chat Completion
```http
POST /chat
Content-Type: application/json

{
    "query": "What services does the organization offer?",
    "conversation_id": "optional-session-id",
    "max_tokens": 512
}
```

#### ğŸ” Semantic Search
```http
POST /search
Content-Type: application/json

{
    "query": "blockchain development",
    "top_k": 5,
    "threshold": 0.7
}
```

#### ğŸ“Š Health Check
```http
GET /health
```

### Response Format
```json
{
    "response": "Generated answer text...",
    "sources": [
        {
            "chunk_id": "chunk_123",
            "content": "Source content...",
            "score": 0.85,
            "metadata": {...}
        }
    ],
    "processing_time": 1.23,
    "conversation_id": "session-uuid"
}
```

---

## ğŸ§ª Evaluation & Testing

### Performance Metrics

```bash
# Run comprehensive evaluation
python eval/eval_rag.py

# Custom evaluation with specific queries
python eval/eval_rag.py --test-file custom_queries.jsonl
```

### Evaluation Results
- **ğŸ“ˆ Precision@K**: Relevance accuracy at top-K results
- **âš¡ Response Time**: Average query processing time
- **ğŸ¯ Intent Accuracy**: Classification performance
- **ğŸ“Š Context Relevance**: Retrieved content quality

### Sample Test Queries
```json
{"query": "What healthcare solutions does the company provide?", "expected_topics": ["healthcare", "medical", "solutions"]}
{"query": "How can I contact the organization?", "expected_topics": ["contact", "phone", "email", "address"]}
{"query": "What technologies do you use for blockchain development?", "expected_topics": ["blockchain", "technology", "development"]}
```

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **ğŸ§  Language Model** | HuggingFace Transformers | Text generation |
| **ğŸ” Embeddings** | Sentence Transformers | Semantic encoding |
| **ğŸ—„ï¸ Vector Store** | FAISS | Similarity search |
| **ğŸ¯ Re-ranking** | Cross-Encoder | Result refinement |
| **ğŸš€ Backend API** | FastAPI + Uvicorn | REST API service |
| **ğŸ–¥ï¸ Frontend** | Streamlit | Interactive UI |
| **ğŸ•·ï¸ Web Scraping** | Scrapy + BeautifulSoup | Data collection |
| **ğŸ“Š Data Processing** | Pandas + NumPy | Data manipulation |
| **ğŸ³ Deployment** | Docker | Containerization |
| **ğŸ“ Logging** | Python Logging | Monitoring |

---

## ğŸ”§ Advanced Configuration

### Custom Data Sources

1. **ğŸ“ Add new data files** to `data/accumulated_data/`
2. **ğŸ”„ Run preprocessing**: `python preprocess.py`
3. **ğŸ§® Rebuild embeddings**: `python embedder.py`
4. **ğŸš€ Restart application**: `python launch.py`

### Model Customization

```python
# config.py - Custom model settings
CUSTOM_MODEL_CONFIG = {
    "embedding_model": "your-custom-embedding-model",
    "llm_model": "your-custom-llm",
    "max_sequence_length": 1024,
    "temperature": 0.7,
    "top_p": 0.9
}
```

### Scaling Configuration

```python
# For high-traffic scenarios
PERFORMANCE_CONFIG = {
    "enable_caching": True,
    "cache_ttl": 3600,  # 1 hour
    "worker_processes": 4,
    "max_concurrent_requests": 100,
    "enable_gpu_batching": True
}
```

---

## ğŸ“Š Monitoring & Analytics

### Query Analytics
- **ğŸ“ˆ Usage patterns** and trending topics
- **âš¡ Performance metrics** and response times
- **ğŸ¯ Accuracy tracking** with user feedback
- **ğŸ’¾ Storage utilization** and index statistics

### Logging Structure
```json
{
    "timestamp": "2025-01-15T10:30:00Z",
    "query": "user question",
    "response_time": 1.23,
    "retrieved_chunks": 5,
    "relevance_scores": [0.89, 0.85, 0.78],
    "session_id": "uuid",
    "user_satisfaction": "positive"
}
```
---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **ğŸ¤— HuggingFace** for transformer models and tools
- **âš¡ FastAPI** for the excellent web framework
- **ğŸ¨ Streamlit** for the intuitive UI framework
- **ğŸ§  Facebook AI** for FAISS vector search
- **ğŸŒ Scrapy** for robust web scraping capabilities

---

<div align="center">


## ğŸ™‹ Author

**Satyaki Mitra**  
*Data Scientist | AI-ML Enthusiast*

---

*Empowering organizations with intelligent, context-aware AI assistance*

[![GitHub](https://img.shields.io/badge/GitHub-Follow-black?style=social&logo=github)](https://github.com/satyaki-mitra)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=social&logo=linkedin)](https://linkedin.com/in/your-profile)

</div>
